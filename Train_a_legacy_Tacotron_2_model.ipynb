{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c9bfcc21c0c47a4bd867e15232e3dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7aaab861e0c4583bf490db783ac3245",
              "IPY_MODEL_33d37455f9264451a8ece6fb1c95604f",
              "IPY_MODEL_b806576213804a88806fad3a2dc3fae1"
            ],
            "layout": "IPY_MODEL_8d1bb65e56724ad38ee7179912969c26"
          }
        },
        "c7aaab861e0c4583bf490db783ac3245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24be79b03a234a838c8f5d635cc53d80",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c6f2ac71d4470282a4e89ed511f918",
            "value": "100%"
          }
        },
        "33d37455f9264451a8ece6fb1c95604f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_629e178b3b3f4eb180ade011e9b36a58",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62aa4e86e7f044a7ba16936f06bb6842",
            "value": 100
          }
        },
        "b806576213804a88806fad3a2dc3fae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_886fd7856e3e4dcda06206c85b3cc6eb",
            "placeholder": "​",
            "style": "IPY_MODEL_494d6c9fe62d417eae5ffc4bcffcbc0d",
            "value": " 100/100 [00:00&lt;00:00, 109.12it/s]"
          }
        },
        "8d1bb65e56724ad38ee7179912969c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24be79b03a234a838c8f5d635cc53d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c6f2ac71d4470282a4e89ed511f918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "629e178b3b3f4eb180ade011e9b36a58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62aa4e86e7f044a7ba16936f06bb6842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "886fd7856e3e4dcda06206c85b3cc6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494d6c9fe62d417eae5ffc4bcffcbc0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dramaticrobotic/Tacotron_2_Legacy/blob/main/Train_a_legacy_Tacotron_2_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODI651CWUdFs"
      },
      "source": [
        "# **A legacy Tacotron 2 model trainer notebook**\n",
        "\n",
        "### Organized and simplified by mega b#6696; edited by Gosmokeless28 | [Tacotron 2](https://github.com/NVIDIA/tacotron2)\n",
        "\n",
        "### Code originally by CookieGalaxy\n",
        "\n",
        "---\n",
        "\n",
        "### (Here is [an output generator notebook](https://colab.research.google.com/drive/1NVA3ndxhYWsKn-zwh3NnzMMgoVdJ5xUx) with which you can test your TT2 model)\n",
        "\n",
        "### **Warning:**  If you're using a non-chromium web browser, you may encounter difficulties if you upload files that are larger than one megabyte in filesize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQBUZTuMogui"
      },
      "source": [
        "# Optional cells. Unhide by clicking the arrow on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75SFQOP_Wnpz",
        "cellView": "form",
        "outputId": "5bc07d8a-c768-4bca-9c4b-7f7f1987ac86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@markdown # Check which GPU you've been allocated\n",
        "#@markdown ### Disconnect from & delete the runtime if you haven't been allocated a desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## It is recommended not to use a K80 GPU\n",
        "\n",
        "!nvidia-smi -L\n",
        "#@markdown All GPUs work, but each of them vary in speed. K80 GPUs are usable, but not recommended.\n",
        "\n",
        "#@markdown ---"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-61041405-2574-1715-742b-986a38b3530d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VKYvFCrUhI4O",
        "outputId": "70523ad9-8325-438d-a165-e0f6cfd096f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "#@markdown # Anti-disconnection\n",
        "#@markdown ## Run this cell to prevent the session from being terminated involuntarily; it will be terminated automatically after 23–24 hours, though.\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Import files\n",
        "#@markdown ### (Step #2 needs to be executed before this cell is ran)\n",
        "#@markdown Instead of using this cell, you can upload the .wav files manually to `tacotron2/wavs` and the `transcript.txt` file to `tacotron2/filelists` using the sidebar on the left.\n",
        "#\n",
        "#@markdown ---\n",
        "#\n",
        "#@markdown #### Where you want to import your dataset from\n",
        "#@markdown This can be a direct download URL, a Google Drive file (URL or ID), a MEGA file (only URL for now), or a local file path (sidebar → right-click file → `Copy path`). **Folders are not supported**, so for this, you need to have your files be in a .zip file.\n",
        "dataset_source = \"C:\\\\Users\\\\drama\\\\Downloads\\\\Angel Dust (Pilot)\\\\Angel Dust (Pilot).zip\" #@param {type:\"string\"}\n",
        "#@markdown #### What type of dataset it is\n",
        "#@markdown The available choices are:\n",
        "#@markdown - **LJ Speech dataset format** - `transcript.txt` and a `wavs` folder.\n",
        "####@markdown - **/mlp/ dataset format** - one folder per .wav, each with `audio.wav` and `label.json`. This format is used by this pile of every My Little Pony voice in existence: https://drive.google.com/drive/folders/10smbrQMZrDoLs5QMgCULVMs9geNdr4W4\n",
        "#@markdown - **Just unzip** - plain .wav files. Works like \"Unzip file to unpack wavs\" from the previous version of this notebook. For this, you need to import the `transcript.txt` manually.\n",
        "loader = \"LJ Speech dataset format\" #@param [\"LJ Speech dataset format\", \"Just unzip\"]\n",
        "#@markdown #### Whether to remove already imported .wav files or not\n",
        "#@markdown Enabled by default. If unchecked, this will combine the current dataset with the new one. Watch out for identical filenames across datasets!\n",
        "remove_current_files = True #@param {type:\"boolean\"}\n",
        "####\n",
        "####@markdown ---\n",
        "####\n",
        "####@markdown ### Data cleanup\n",
        "####@markdown Note: these are pretty slow, can't be stopped without restarting the runtime, and you probably don't need them if the dataset is good.\n",
        "####@markdown #### Maximum volume counted as silence for removal (percentage)\n",
        "####@markdown `-1` (disabled) by default. The loader can remove silence from the start and end of the audio, to prevent the AI from having to spend precious neurons on memorizing the bounds of each clip. **Recommended** for the /mlp/ datasets, with a value of `0.1` (as used in [one of their notebooks](https://colab.research.google.com/drive/1Tv6yaMQ0rxX9Zru3_D16Yzp5gQNsgn9h#scrollTo=05SOBkCj7Lrm)). If using with other datasets, zoom in on one of the audio files with Audacity to check what level the silence is.\n",
        "###silence_max =  -1#@param {type:\"number\"}\n",
        "####@markdown #### Whether to normalize the audio\n",
        "####@markdown Disabled by default. This sets the audio volume for each clip as high as it can be without going too far, which might be bad if some of your clips are quiet for a reason (e.g. whispering). You probably don't need this anyway, though.\n",
        "###normalize = False #@param {type:\"boolean\"}\n",
        "####\n",
        "####@markdown ---\n",
        "####\n",
        "####@markdown ### /mlp/ importer only\n",
        "####@markdown #### Maximum noisiness level\n",
        "####@markdown Each file is marked by how clean it is. For characters with less voicelines, you might want to loosen your restriction. You'll see the amounts of each type of clip when you run the cell. Your download is cached, so just re-run the cell with different settings if you want to change this.\n",
        "###mlp_max_noise = \"Noisy\" #@param [\"Clean\", \"Noisy\", \"Very Noisy\"]\n",
        "####@markdown #### Allowed emotions\n",
        "####@markdown Empty (allowing all) by default. Each file also has one or more emotion tags. Write down the ones you want, separated by commas. Note that only one of the clip's emotion tags needs to match, so a clip with `Happy, Shouting` will be allowed even if you only specified `Happy`.\n",
        "####\n",
        "####@markdown Common emotion tags (taken from [here](https://docs.google.com/document/d/1xe1Clvdg6EFFDtIkkFwT-NPLRDPvkV4G675SUKjxVRU/edit#heading=h.dt6j5eyzo9id)): `Neutral, Happy, Amused, Sad, Annoyed, Angry, Disgust, Sarcastic, Smug, Fear, Anxious, Confused, Surprised, Tired, Whispering, Shouting, Whining, Crazy`\n",
        "###mlp_allowed_emotions = \"\" #@param {type:\"string\"}\n",
        "####@markdown #### Disallowed emotions\n",
        "####@markdown `Whispering` by default. Same deal as above, just the other way around.\n",
        "###mlp_disallowed_emotions = \"Whispering\" #@param {type:\"string\"}\n",
        "\n",
        "import gdown\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "###!apt install sox\n",
        "\n",
        "os.chdir(\"/content/tacotron2\")\n",
        " \n",
        "if not os.path.isdir(\"/content/tacotron2/wavs\"):\n",
        "  raise Exception(\"You must run step 1 before running this.\")\n",
        " \n",
        "wavs_path = \"/content/tacotron2/wavs\"\n",
        "filelist_path = \"/content/tacotron2/filelists/list.txt\"\n",
        " \n",
        "def add_to_list_txt(to_add):\n",
        "  with open(filelist_path, \"a+\") as f:\n",
        "    if not f.seek(0,2) == \"\\n\":\n",
        "      f.write(\"\\n\")\n",
        "    f.write(to_add)\n",
        " \n",
        "if remove_current_files:\n",
        "  !rm -rf {wavs_path}\n",
        "  !mkdir {wavs_path}\n",
        "  !rm -f {filelist_path}\n",
        "\n",
        "!rm -rf /content/tempwavs\n",
        "!rm -rf /content/tempwavs2\n",
        "\n",
        "try:\n",
        "  latest_downloaded\n",
        "except NameError:\n",
        "  latest_downloaded = None\n",
        "\n",
        "if latest_downloaded == dataset_source:\n",
        "  print(\"%s is already downloaded\" % dataset_source)\n",
        "else:\n",
        "  latest_downloaded = dataset_source\n",
        "  !rm -f /content/dataset\n",
        " \n",
        "  m = re.search(\"^https?://drive\\.google\\.com/file/d/([a-zA-Z0-9\\-_]+)/\", dataset_source)\n",
        "  if m:\n",
        "    dataset_source = m.group(1)\n",
        " \n",
        "  if dataset_source.startswith(\"http\"):\n",
        "    if dataset_source.startswith(\"https://mega.nz/\"):\n",
        "      !/content/tacotron2/megadown.sh {dataset_source} -o /content/dataset\n",
        "    else:\n",
        "      !curl -s -L $dataset_source -o /content/dataset\n",
        "  elif dataset_source.startswith(\"/\"):\n",
        "    !cp $dataset_source /content/dataset\n",
        "  else:\n",
        "    gdown.download(\"https://drive.google.com/uc?id=\" + dataset_source, \"/content/dataset\", quiet = False)\n",
        " \n",
        "!7z x /content/dataset -o/content/datasetunzip\n",
        "\n",
        "if loader == \"LJSpeech-style dataset\":\n",
        "  #!unzip -q /content/dataset -d /content/datasetunzip\n",
        "  !mv /content/datasetunzip/wavs /content/tempwavs\n",
        "  if remove_current_files:\n",
        "    !mv /content/datasetunzip/list.txt {filelist_path}\n",
        "  else:\n",
        "    with open(\"/content/datasetunzip/list.txt\") as f:\n",
        "      add_to_list_txt(f.read())\n",
        "  #!rm -r /content/datasetunzip\n",
        " \n",
        "###elif loader == \"/mlp/ dataset\":\n",
        "###  #!mkdir /content/datasetuntar\n",
        "###  #!tar -xf /content/dataset -C /content/datasetuntar\n",
        "###  !mkdir /content/tempwavs\n",
        "###  list_txt = []\n",
        "###  noise_chart = {\"\": 0, \"Clean\": 0, \"Noisy\": 1, \"Very Noisy\": 2}\n",
        "###  noise_stats = {}\n",
        "###  emotion_stats = {}\n",
        "###  mlp_allowed_emotions = [i.strip() for i in mlp_allowed_emotions.split(\",\")] if mlp_allowed_emotions else []\n",
        "###  mlp_disallowed_emotions = [i.strip() for i in mlp_disallowed_emotions.split(\",\")] if mlp_disallowed_emotions else []\n",
        "###  counter = 0\n",
        "###  for dirpath, dirnames, filenames in os.walk(\"/content/datasetunzip\"):\n",
        "###    for i in dirnames:\n",
        "###      with open(os.path.join(dirpath, i, \"label.json\")) as f:\n",
        "###        data = json.load(f)\n",
        "###        noise_stats[data[\"noise\"]] = noise_stats.get(data[\"noise\"], 0) + 1\n",
        "###        for j in data[\"tags\"]:\n",
        "###          emotion_stats[j] = emotion_stats.get(j, 0) + 1\n",
        "###        if noise_chart[data[\"noise\"]] <= noise_chart[mlp_max_noise]:\n",
        "###          if len(mlp_allowed_emotions) == 0 or any([i in mlp_allowed_emotions for i in data[\"tags\"]]):\n",
        "###            if not any([i in mlp_disallowed_emotions for i in data[\"tags\"]]):\n",
        "###              list_txt.append(\"wavs/\" + i + \".wav|\" + data[\"utterance\"][\"content\"])\n",
        "###              os.rename(os.path.join(dirpath, i, \"audio.wav\"), os.path.join(\"/content/tempwavs\", i + \".wav\"))\n",
        "###              counter += 1\n",
        "###  if remove_current_files:\n",
        "###    with open(filelist_path, \"w\") as f:\n",
        "###      f.write(\"\\n\".join(list_txt))\n",
        "###  else:\n",
        "###    add_to_list_txt(\"\\n\".join(list_txt))\n",
        "###  #!rm -r /content/datasetuntar\n",
        "###  print(noise_stats)\n",
        "###  print(\"\\n\".join([\" \".join([str(j) for j in i]) for i in sorted(emotion_stats.items(), key = lambda x:x[1], reverse = True)]))\n",
        "###  print(f\"{counter} loaded\")\n",
        " \n",
        "elif loader == \"Just unzip\":\n",
        "  #!unzip -q /content/dataset -d /content/tempwavs\n",
        "  !mv /content/datasetunzip /content/tempwavs\n",
        " \n",
        "!rm -r /content/datasetunzip\n",
        " \n",
        "###if silence_max != -1:\n",
        "###  print(\"Trimming silence...\")\n",
        "###  !mkdir /content/tempwavs2\n",
        "###  for i in tqdm(list(os.scandir(\"/content/tempwavs\"))):\n",
        "###    !sox {i.path} {os.path.join(\"/content/tempwavs2\", i.name)} silence 1 0.05 {silence_max}% reverse silence 1 0.05 {silence_max}% reverse\n",
        "###  !rm -rf /content/tempwavs\n",
        "###  !mv /content/tempwavs2 /content/tempwavs\n",
        " \n",
        "###if normalize:\n",
        "###  print(\"Normalizing...\")\n",
        "###  !mkdir /content/tempwavs2\n",
        "###  for i in tqdm(list(os.scandir(\"/content/tempwavs\"))):\n",
        "###    !sox {i.path} {os.path.join(\"/content/tempwavs2\", i.name)} gain -h\n",
        "###  !rm -rf /content/tempwavs\n",
        "###  !mv /content/tempwavs2 /content/tempwavs\n",
        " \n",
        "# https://unix.stackexchange.com/a/626625\n",
        "!cp --force --archive --update --link /content/tempwavs/. {wavs_path}\n",
        "!rm -rf /content/tempwavs\n",
        " \n",
        "print(\"tacotron2/wavs now contains %s files\" % len(os.listdir(wavs_path)))"
      ],
      "metadata": {
        "id": "UqJYYqzyxkEj",
        "cellView": "form",
        "outputId": "bd2dfc99-fc35-4d26-df95-9a96f69fbac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\drama\\Downloads\\Angel Dust (Pilot)\\Angel Dust (Pilot).zip is already downloaded\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "ERROR: No more files\n",
            "/content/dataset\n",
            "\n",
            "\n",
            "\n",
            "System ERROR:\n",
            "Unknown error -2147024872\n",
            "rm: cannot remove '/content/datasetunzip': No such file or directory\n",
            "cp: cannot stat '/content/tempwavs/.': No such file or directory\n",
            "tacotron2/wavs now contains 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhZSeuWHoj6g"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6aq6vRzqEV9",
        "cellView": "form",
        "outputId": "fbf3624e-7054-4a35-83c4-b1ff18e0be89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@markdown ## **1** Mount Google Drive\n",
        "\n",
        "#Google Drive Authentication Token\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6zQ-DWo4lcs",
        "cellView": "form",
        "outputId": "ec02e9d7-9f44-4165-aa6d-ddacf02a8584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@markdown ## **2** After running this cell, insert the wavs—required to be in 22,050 Hz, mono, and 16 bit—into the /content/tacotron2/wavs folder on the left.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## If the dataset contains a lot of .wav files, you should use the optional cell, \"Import files\", after running this.\n",
        "\n",
        "#@markdown #### Execute this step to install Tacotron 2 and dependencies\n",
        "!pip install git+https://github.com/justinjohn0306/gdown.git\n",
        "import os\n",
        "!git clone -q https://github.com/NVIDIA/tacotron2\n",
        "os.chdir('tacotron2')\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install -q unidecode tensorboardX\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/justinjohn0306/gdown.git\n",
            "  Cloning https://github.com/justinjohn0306/gdown.git to /tmp/pip-req-build-oc082uxx\n",
            "  Running command git clone -q https://github.com/justinjohn0306/gdown.git /tmp/pip-req-build-oc082uxx\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.5.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown==4.5.3) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.5.3) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.5.3) (3.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.5.3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.5.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.5.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.5.3) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.5.3) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.5.3) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.3-py3-none-any.whl size=14857 sha256=a4d39cf062b0d34d14eae1063448558389cb483ceae26c5a1933ddf643cd33cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g16lejhk/wheels/b8/fd/2e/577ecbc3fc8775dd17e609dffdcad1ebd61ebe70bea9dd0c2b\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.3\n",
            "Submodule 'waveglow' (https://github.com/NVIDIA/waveglow) registered for path 'waveglow'\n",
            "Cloning into '/content/tacotron2/waveglow'...\n",
            "Submodule path 'waveglow': checked out '5bc2a53e20b3b533362f974cfa1ea0267ae1c2b1'\n",
            "\u001b[K     |████████████████████████████████| 235 kB 14.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 75.2 MB/s \n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  doc-base\n",
            "The following NEW packages will be installed:\n",
            "  pv\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 48.3 kB of archives.\n",
            "After this operation, 123 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 pv amd64 1.6.6-1 [48.3 kB]\n",
            "Fetched 48.3 kB in 0s (783 kB/s)\n",
            "Selecting previously unselected package pv.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/pv_1.6.6-1_amd64.deb ...\n",
            "Unpacking pv (1.6.6-1) ...\n",
            "Setting up pv (1.6.6-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libjq1 libonig4\n",
            "The following NEW packages will be installed:\n",
            "  jq libjq1 libonig4\n",
            "0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 276 kB of archives.\n",
            "After this operation, 930 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n",
            "Fetched 276 kB in 0s (2,743 kB/s)\n",
            "Selecting previously unselected package libonig4:amd64.\n",
            "(Reading database ... 124028 files and directories currently installed.)\n",
            "Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n",
            "Unpacking libonig4:amd64 (6.7.0-1) ...\n",
            "Selecting previously unselected package libjq1:amd64.\n",
            "Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Selecting previously unselected package jq.\n",
            "Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n",
            "Unpacking jq (1.5+dfsg-2) ...\n",
            "Setting up libonig4:amd64 (6.7.0-1) ...\n",
            "Setting up libjq1:amd64 (1.5+dfsg-2) ...\n",
            "Setting up jq (1.5+dfsg-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "--2023-01-01 04:56:50--  https://raw.githubusercontent.com/tonikelope/megadown/master/megadown\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16684 (16K) [text/plain]\n",
            "Saving to: ‘megadown.sh’\n",
            "\n",
            "megadown.sh         100%[===================>]  16.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-01 04:56:50 (129 MB/s) - ‘megadown.sh’ saved [16684/16684]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title monkey patch for tensorflow 2\n",
        "%%writefile hparams.py\n",
        "import tensorflow as tf\n",
        "from text import symbols\n",
        "\n",
        "class HParamsAlternative(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(HParamsAlternative, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def create_hparams(hparams_string=None, verbose=False):\n",
        "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
        "\n",
        "    hparams = HParamsAlternative(\n",
        "        ################################\n",
        "        # Experiment Parameters        #\n",
        "        ################################\n",
        "        epochs=500,\n",
        "        iters_per_checkpoint=1000,\n",
        "        seed=1234,\n",
        "        dynamic_loss_scaling=True,\n",
        "        fp16_run=False,\n",
        "        distributed_run=False,\n",
        "        dist_backend=\"nccl\",\n",
        "        dist_url=\"tcp://localhost:54321\",\n",
        "        cudnn_enabled=True,\n",
        "        cudnn_benchmark=False,\n",
        "        ignore_layers=['embedding.weight'],\n",
        "\n",
        "        ################################\n",
        "        # Data Parameters             #\n",
        "        ################################\n",
        "        load_mel_from_disk=False,\n",
        "        training_files='filelists/ljs_audio_text_train_filelist.txt',\n",
        "        validation_files='filelists/ljs_audio_text_val_filelist.txt',\n",
        "        text_cleaners=['basic_cleaners'],\n",
        "\n",
        "        ################################\n",
        "        # Audio Parameters             #\n",
        "        ################################\n",
        "        max_wav_value=32768.0,\n",
        "        sampling_rate=22050,\n",
        "        filter_length=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        n_mel_channels=80,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax=8000.0,\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        n_symbols=len(symbols),\n",
        "        symbols_embedding_dim=512,\n",
        "\n",
        "        # Encoder parameters\n",
        "        encoder_kernel_size=5,\n",
        "        encoder_n_convolutions=3,\n",
        "        encoder_embedding_dim=512,\n",
        "\n",
        "        # Decoder parameters\n",
        "        n_frames_per_step=1,  # currently only 1 is supported\n",
        "        decoder_rnn_dim=1024,\n",
        "        prenet_dim=256,\n",
        "        max_decoder_steps=1000,\n",
        "        gate_threshold=0.5,\n",
        "        p_attention_dropout=0.1,\n",
        "        p_decoder_dropout=0.1,\n",
        "\n",
        "        # Attention parameters\n",
        "        attention_rnn_dim=1024,\n",
        "        attention_dim=128,\n",
        "\n",
        "        # Location Layer parameters\n",
        "        attention_location_n_filters=32,\n",
        "        attention_location_kernel_size=31,\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        postnet_embedding_dim=512,\n",
        "        postnet_kernel_size=5,\n",
        "        postnet_n_convolutions=5,\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        use_saved_learning_rate=False,\n",
        "        learning_rate=1e-3,\n",
        "        weight_decay=1e-6,\n",
        "        grad_clip_thresh=1.0,\n",
        "        batch_size=64,\n",
        "        mask_padding=True  # set model's padded outputs to padded values\n",
        "    )\n",
        "\n",
        "    if hparams_string:\n",
        "        tf.logging.info('Parsing command line hparams: %s', hparams_string)\n",
        "        hparams.parse(hparams_string)\n",
        "\n",
        "    if verbose:\n",
        "        tf.logging.info('Final parsed hparams: %s', hparams.values())\n",
        "\n",
        "    return hparams"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5m8GBhYFkvRN",
        "outputId": "45900f3d-1268-4001-c636-9484b29b2a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hparams.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download the base model\n",
        "%matplotlib inline\n",
        "import os\n",
        "if os.getcwd() != '/content/tacotron2':\n",
        "    os.chdir('tacotron2')\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        " \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "#from tqdm import tqdm # Terminal\n",
        "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    import random\n",
        "    if random.random() > 0.85:\n",
        "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "            iteration, filepath))\n",
        "        try:\n",
        "            torch.save({'iteration': iteration,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'learning_rate': learning_rate}, filepath)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "        print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"/content/tacotron2/pretrained_model\")\n",
        "      %cd /dev/null\n",
        "      !/content/tacotron2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n",
        "      %cd /content/tacotron2\n",
        "      model = warm_start_model(\"/content/tacotron2/pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "    \n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "warm_start=False#sorry bout that\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "%cd /content/tacotron2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "axmk42gDkiQf",
        "outputId": "254a4253-7bce-4428-ca6d-56a66996f852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
            "/content/tacotron2/text/__init__.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  return s in _symbol_to_id and s is not '_' and s is not '~'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sed: can't read filelists/clipper_train_filelist.txt: No such file or directory\n",
            "sed: can't read filelists/clipper_val_filelist.txt: No such file or directory\n",
            "/content\n",
            "mkdir: cannot create directory ‘wavs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP1trdpN_jV6",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## **3** Set the model parameters\n",
        "#@markdown ---\n",
        "#@markdown #### The name of the TT2 model\n",
        "model_filename = 'Tacotron_2_model' #@param {type: \"string\"}\n",
        "\n",
        "#@markdown #### Import the transcript .txt file into the /content/tacotron2/filelists folder on the left.\n",
        "Training_file = \"filelists/transcript.txt\" #@param {type: \"string\"}\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "\n",
        "# hparams to Tune\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "# Learning Rate             # https://www.desmos.com/calculator/ptgcz4vzsw / https://cdn.discordapp.com/attachments/841461499946860576/993611452499902504/scrnli_7_4_2022_1-16-48_PM.png\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "#@markdown #### The amount of epochs to train the model for.\n",
        "#@markdown ###### You probably should not change this parameter, one of the reasons being that training a legacy TT2 model for an unnecessarily larger epoch amount does **not** result in it being better ultimately (Read the batch size paragraph).\n",
        "hparams.epochs = 200 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown #### The batch size. Lower if you don't have enough RAM.\n",
        "#@markdown ###### If the GPU you've been allocated is a K80, use a batch size that is no larger than `16`, and if the GPU you've been allocated is a P100, use a batch size that is no larger than `32`.\n",
        "#@markdown ###### Furthermore, training the model with a low batch size will give it more time to learn in the course of it being trained.\n",
        "#@markdown ###### In particular, `4` is the ideal batch size to use when it comes to training a non-ARPAbet Tacotron 2 model, and the batch size of `8` should be used for training an ARPAbet TT2 model.\n",
        "#@markdown ###### However, even though training a model with a low batch size gives it more time to learn, training a model with a batch size that is **too** low causes it to learn **too much** in the course of it being trained, which consequently overfits it.\n",
        "#@markdown ###### That outcome is also what occurs as a result of a model being trained for too many epochs. If you're not sure about which one of the aforementioned batch sizes to choose to train the model with, use the preset batch size, which is `8` by default.\n",
        "hparams.batch_size = 8 #@param {type: \"integer\"}\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "\n",
        "#@markdown #### The learning rate and the minimum learning rate\n",
        "hparams.learning_rate = 5e-4 #@param\n",
        "hparams.min_learning_rate = 1e-5 #@param\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "\n",
        "#@markdown #### Where to save the model when training\n",
        "output_directory = '/content/drive/MyDrive/Tacotron 2/models' #@param {type: \"string\"}\n",
        "log_directory = '/content/tacotron2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/MyDrive/Tacotron 2/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "#@markdown ---"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2-xc6EcACWc",
        "cellView": "form",
        "outputId": "ce389c99-81b1-461a-8224-607ed19db1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482,
          "referenced_widgets": [
            "5c9bfcc21c0c47a4bd867e15232e3dda",
            "c7aaab861e0c4583bf490db783ac3245",
            "33d37455f9264451a8ece6fb1c95604f",
            "b806576213804a88806fad3a2dc3fae1",
            "8d1bb65e56724ad38ee7179912969c26",
            "24be79b03a234a838c8f5d635cc53d80",
            "b7c6f2ac71d4470282a4e89ed511f918",
            "629e178b3b3f4eb180ade011e9b36a58",
            "62aa4e86e7f044a7ba16936f06bb6842",
            "886fd7856e3e4dcda06206c85b3cc6eb",
            "494d6c9fe62d417eae5ffc4bcffcbc0d"
          ]
        }
      },
      "source": [
        "#@markdown ## **4** Convert the wavs into mel spectrograms\n",
        "#@markdown ### This cell also checks for missing files\n",
        "print(\"Generating mels\")\n",
        "if generate_mels:\n",
        "    create_mels()\n",
        "\n",
        "print(\"Checking for missing files\")\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' \"{hparams.training_files}\"; sed -i -- 's,.wav|,.npy|,g' \"{hparams.validation_files}\"\n",
        "\n",
        "check_dataset(hparams)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating mels\n",
            "Generating Mels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c9bfcc21c0c47a4bd867e15232e3dda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for missing files\n",
            "Checking Training Files\n",
            " \n",
            "[WARNING] does not exist.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-37fdb9b04f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sed -i -- \\'s,.wav|,.npy|,g\\' \"{hparams.training_files}\"; sed -i -- \\'s,.wav|,.npy|,g\\' \"{hparams.validation_files}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcheck_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-4c1529df97c2>\u001b[0m in \u001b[0;36mcheck_dataset\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking Training Files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0maudiopaths_and_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_filepaths_and_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get split lines from training_files text file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mcheck_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudiopaths_and_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking Validation Files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0maudiopaths_and_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_filepaths_and_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get split lines from validation_files text file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4c1529df97c2>\u001b[0m in \u001b[0;36mcheck_arr\u001b[0;34m(filelist_arr)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n[WARNING] does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n[info] has no/very little text.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34mr\"!?,.;:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqIk1dIuCqhx",
        "cellView": "form"
      },
      "source": [
        "#@markdown # **5** Train the model\n",
        "\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ql3OWWPKZv"
      },
      "source": [
        "# **Now, the eggheads think this is what good training looks like:**\n",
        "\n",
        "![img.png](https://media.discordapp.net/attachments/835971020569051216/851469553355587614/download_2.png)\n",
        "\n",
        "# **But I think... it looks more like this:**\n",
        "\n",
        "![img.png](https://cdn.discordapp.com/attachments/625091103887982602/992537047879393450/Screenshotter--YouTube-KingOfTheHillAdsStricklandMessageadultswimhistory-008.jpg)\n",
        "\n",
        "JS to prevent idle timeout:\n",
        "\n",
        "Press F12 OR CTRL + SHIFT + I OR right click on this website -> inspect;\n",
        "then click on the console tab and paste in the following code:\n",
        "\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    }
  ]
}